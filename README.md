# Transformer from Scratch in NumPy

This repository contains a simple implementation of a Transformer architecture using only NumPy. The model is implemented purely in NumPy and does not include backpropagation. It is designed for educational purposes to understand Transformer mechanics. Only small sequences and a small vocabulary are supported.

## Features

- Custom token embeddings and positional encodings  
- Multi-head self-attention and masked attention  
- Feed-forward layers with ReLU activation  
- Layer normalization  
- Linear output layer with softmax for token prediction  
- Fully implemented encoder and decoder pipeline  
